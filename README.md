# Avito-Test

## Как запустить

1) (опционально) создать окружение <br>
   ```python -m venv .venv && source .venv/bin/activate```
2) установить зависимости <br>
   ```pip install -r requirements.txt```
3) перенести ваш датасет в корень проекта
4) запустить инференс <br>
   ```python inference.py -i dataset.csv -o submission.csv``` <br>
   или <br>
   ```python inference.py -i dataset.txt -o submission.csv```


## Навигация

```research.ipynb``` - процесс обучения и исследования <br>
```inference.py``` - инференс модели


## Подход к решению

Цель: восстановить пропуски между словами в слитном тексте, оптимизируя F1 по позициям пробелов. Быстро и дешево, без LLM.

Данные/разметка:
- Корпус: русская Википедия.
- Для каждого предложения: убираем пробелы → text_no_spaces; метки boundaries — индексы позиций пробелов в исходнике.
- Класс-имбаланс: даунсэмпл негативов + веса при обучении.

Признаки (окно ±5 символов):
- Символы слева/справа, категории (cyr/lat/dig/pnc), переходы категорий, регистр.
- Локальные «токены» слева/справа и их длины.
- Лексиконные флаги по униграммам (наличие/порог частоты).

Лексикон (униграммы):
- Частоты слов из ru-Wiki (unigram_freq.tsv). Используются как фичи и как подсказка «словности».

Модель:
- Логистическая регрессия (SGDClassifier, hashing-trick с фиксированным n_features).
- Обучение стриминговое (partial_fit), несколько эпох, сид фиксирован.

Комбинация с униграммами (опционально, без ретрейна):
- score(pos) = logit(p_LR(pos)) + λ * (log f(lt) + log f(rt) − log f(merged)), со сглаживанием.
- λ подбирается на валидации.

Постобработка (правила):
- Не ставить пробел внутри паттернов: 1920x1080, 16:9, 12:30, даты (10.09.2025), десятичные (3,14), б/у, телефоны, email/URL.

Инференс:
- Для текста считаем proba для всех позиций, применяем порог из best_threshold.txt.
- Формат вывода: строка вида "[5, 8, 13]" в колонке predicted_positions.
