# Avito-Test

## КАК ЗАПУСТИТЬ

1) (опционально) создать окружение
   python -m venv .venv && source .venv/bin/activate
2) установить зависимости
   pip install < requirements.txt
3) запустить инференс
   python inference.py -i dataset.csv -o submission.csv
   или
   python inference.py -i dataset_XXXX.txt -о submission.csv

## ВЫХОД

submission.csv — копия входного файла с добавленной колонкой predicted_positions
в формате строки: [5, 8, 13]

## НАВИГАЦИЯ

research.ipynb - процесс обучения и исследования
inference.py - инференс модели


## ПОДХОД

Цель: восстановить пропуски между словами в слитном тексте, оптимизируя F1 по позициям пробелов. Быстро и дешево, без LLM.

Данные/разметка:
- Корпус: русская Википедия.
- Для каждого предложения: убираем пробелы → text_no_spaces; метки boundaries — индексы позиций пробелов в исходнике.
- Класс-имбаланс: даунсэмпл негативов + веса при обучении.

Признаки (окно ±5 символов):
- Символы слева/справа, категории (cyr/lat/dig/pnc), переходы категорий, регистр.
- Локальные «токены» слева/справа и их длины.
- Лексиконные флаги по униграммам (наличие/порог частоты).

Лексикон (униграммы):
- Частоты слов из ru-Wiki (unigram_freq.tsv). Используются как фичи и как подсказка «словности».

Модель:
- Логистическая регрессия (SGDClassifier, hashing-trick с фиксированным n_features).
- Обучение стриминговое (partial_fit), несколько эпох, сид фиксирован.

Комбинация с униграммами (опционально, без ретрейна):
- score(pos) = logit(p_LR(pos)) + λ * (log f(lt) + log f(rt) − log f(merged)), со сглаживанием.
- λ подбирается на валидации.

Постобработка (правила):
- Не ставить пробел внутри паттернов: 1920x1080, 16:9, 12:30, даты (10.09.2025), десятичные (3,14), б/у, телефоны, email/URL.

Инференс:
- Для текста считаем proba для всех позиций, применяем порог из best_threshold.txt.
- Формат вывода: строка вида "[5, 8, 13]" в колонке predicted_positions.
